{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Necessary Imports Needed to Run Model"
      ],
      "metadata": {
        "id": "wPtKnbAlevGs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ve_uMRiaehEA"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import random\n",
        "import copy\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "from sklearn.utils import shuffle\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager\n",
        "from collections import OrderedDict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive and Extract Necessary Folders"
      ],
      "metadata": {
        "id": "2c2wlpoze3Mg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSQl0lsDdUL4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwXMaGmS0vEa"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpzLvBkszOTA"
      },
      "outputs": [],
      "source": [
        "# define which percent data you want to use\n",
        "data_folder = '100percent'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DS-UcA-Pb0BO"
      },
      "outputs": [],
      "source": [
        "# Unzipping the Food-101 dataset - training\n",
        "zip_file_path = '/content/drive/MyDrive/EC523-Project/data/zip/food101_' + data_folder + '.zip'\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/train')\n",
        "\n",
        "# Unzipping the Food-101 dataset - testing\n",
        "zip_file_path = '/content/drive/MyDrive/EC523-Project/data/zip/food101_test.zip'\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/train')\n",
        "\n",
        "classes = sorted(os.listdir('/content/train/food101_' + data_folder + '/train'))\n",
        "print(len(classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_l8Ln0QfsQH"
      },
      "outputs": [],
      "source": [
        "# Assuming the Food-101 dataset is unzipped in '/content/food-101'\n",
        "train_images_path = '/content/train/food101_' + data_folder + '/train'\n",
        "test_images_path = '/content/train/food101_test'\n",
        "# Lists to store training and testing data paths\n",
        "train_data = []\n",
        "test_data = []\n",
        "\n",
        "\n",
        "for food_category in os.listdir(train_images_path):\n",
        "    category_path = os.path.join(train_images_path, food_category)\n",
        "    images = os.listdir(category_path)\n",
        "    train_images = images[:]\n",
        "\n",
        "    # Add image paths to the train and test lists\n",
        "    train_data.extend([f\"{food_category}/{img}\" for img in train_images])\n",
        "\n",
        "for food_category in os.listdir(test_images_path):\n",
        "    category_path = os.path.join(test_images_path, food_category)\n",
        "    images = os.listdir(category_path)\n",
        "    test_images = images[:]\n",
        "    # Add image paths to the train and test lists\n",
        "    test_data.extend([f\"{food_category}/{img}\" for img in test_images])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a Meta Folder to Emulate Pytorch Dataset"
      ],
      "metadata": {
        "id": "jDF2h2YQgND4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory_path = '/content/train/meta'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(directory_path):\n",
        "    os.makedirs(directory_path)\n",
        "    print(f\"Directory '{directory_path}' created.\")\n",
        "\n",
        "# File path for the file to create within the directory\n",
        "file_path = os.path.join(directory_path, 'train.txt')\n",
        "file_path = os.path.join(directory_path, 'test.txt')\n",
        "file_path = os.path.join(directory_path, 'classes.txt')\n",
        "\n",
        "\n",
        "# Write the train and test data to files\n",
        "with open('/content/train/meta/train.txt', 'w') as f:\n",
        "    for item in train_data:\n",
        "        f.write(\"%s\\n\" % item)\n",
        "\n",
        "with open('/content/train/meta/test.txt', 'w') as f:\n",
        "    for item in test_data:\n",
        "        f.write(\"%s\\n\" % item)\n",
        "\n",
        "with open('/content/train/meta/classes.txt', 'w') as f:\n",
        "    for item in classes:\n",
        "        f.write(\"%s\\n\" % item)\n",
        "\n",
        "# Display the first 5 lines of each file as a sample\n",
        "print(\"Testing images\")\n",
        "print(\"\\n\".join(test_data[:5]))\n",
        "print(\"\\nTraining images\")\n",
        "print(\"\\n\".join(train_data[:5]))"
      ],
      "metadata": {
        "id": "s5Lm5mRWgC4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANo9Po4I6vbO"
      },
      "outputs": [],
      "source": [
        "#Check Meta Folder\n",
        "os.listdir('/content/train/meta')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Data Frame"
      ],
      "metadata": {
        "id": "d3cWL7W4gZZX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OI8AAiy80pJC"
      },
      "outputs": [],
      "source": [
        "def prep_df(path: str) -> pd.DataFrame:\n",
        "    array = open(path, 'r').read().splitlines()\n",
        "\n",
        "    # Getting the full path for the images\n",
        "    img_path = \"/content/train/food101_\" + data_folder + \"/train/\"\n",
        "    full_path = [img_path + img for img in array]\n",
        "\n",
        "    # Splitting the image index from the label\n",
        "    imgs = []\n",
        "    for img in array:\n",
        "        img = img.split('/')\n",
        "\n",
        "        imgs.append(img)\n",
        "\n",
        "    imgs = np.array(imgs)\n",
        "    # Converting the array to a data frame\n",
        "    imgs = pd.DataFrame(imgs[:,0], imgs[:,1], columns=['label'])\n",
        "    # Adding the full path to the data frame\n",
        "    imgs['path'] = full_path\n",
        "\n",
        "    # Randomly shuffling the order to the data in the dataframe\n",
        "    imgs = shuffle(imgs)\n",
        "\n",
        "    return imgs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtSLcK_E_acU"
      },
      "outputs": [],
      "source": [
        "def prep_dftest(path: str) -> pd.DataFrame:\n",
        "    array = open(path, 'r').read().splitlines()\n",
        "\n",
        "    # Getting the full path for the images\n",
        "    img_path = \"/content/train/food101_test/\"\n",
        "    full_path = [img_path + img for img in array]\n",
        "\n",
        "    # Splitting the image index from the label\n",
        "    imgs = []\n",
        "    for img in array:\n",
        "        img = img.split('/')\n",
        "\n",
        "        imgs.append(img)\n",
        "\n",
        "    imgs = np.array(imgs)\n",
        "    # Converting the array to a data frame\n",
        "    imgs = pd.DataFrame(imgs[:,0], imgs[:,1], columns=['label'])\n",
        "    # Adding the full path to the data frame\n",
        "    imgs['path'] = full_path\n",
        "\n",
        "    # Randomly shuffling the order to the data in the dataframe\n",
        "    imgs = shuffle(imgs)\n",
        "\n",
        "    return imgs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1PwEyz52gXS"
      },
      "outputs": [],
      "source": [
        "train_imgs = prep_df('/content/train/meta/train.txt')\n",
        "test_imgs = prep_dftest('/content/train/meta/test.txt')\n",
        "\n",
        "train_imgs.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3RR74Q7_tmK"
      },
      "outputs": [],
      "source": [
        "test_imgs.head(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gu6oAi3P3FmQ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "num_rows = 3\n",
        "num_cols = 8\n",
        "\n",
        "\n",
        "for idx in range(num_rows * num_cols):\n",
        "    random_idx = np.random.randint(0, train_imgs.shape[0])\n",
        "    img = plt.imread(train_imgs.path.iloc[random_idx])\n",
        "\n",
        "    label = train_imgs.label.iloc[random_idx]\n",
        "\n",
        "    ax = plt.subplot(num_rows, num_cols, idx + 1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(label)\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform Data Augmentation"
      ],
      "metadata": {
        "id": "eHXaHzQ-giXn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZZZxcIP4C-q"
      },
      "outputs": [],
      "source": [
        "# Data augmentation for training\n",
        "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
        "                                       transforms.RandomResizedCrop(224),\n",
        "                                       transforms.RandomHorizontalFlip(),\n",
        "                                       torchvision.transforms.AutoAugment(torchvision.transforms.AutoAugmentPolicy.IMAGENET),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                            [0.229, 0.224, 0.225])])\n",
        "# Data augmentation for testing\n",
        "test_transforms = transforms.Compose([transforms.Resize(255),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                           [0.229, 0.224, 0.225])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pm4HyfuD4KbB"
      },
      "outputs": [],
      "source": [
        "class Label_encoder:\n",
        "    def __init__(self, labels):\n",
        "        labels = list(set(labels))\n",
        "        self.labels = {label: idx for idx, label in enumerate(classes)}\n",
        "\n",
        "    def get_label(self, idx):\n",
        "        return list(self.labels.keys())[idx]\n",
        "\n",
        "    def get_idx(self, label):\n",
        "        return self.labels[label]\n",
        "\n",
        "encoder = Label_encoder(classes)\n",
        "for i in range(101):\n",
        "    print(encoder.get_label(i), encoder.get_idx( encoder.get_label(i) ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfu6KLyD4Xfa"
      },
      "outputs": [],
      "source": [
        "class Food101(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataframe.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.dataframe.path.iloc[idx]\n",
        "        image = Image.open(img_name)\n",
        "        if image.mode != 'RGB':\n",
        "            image = image.convert('RGB')\n",
        "        label = encoder.get_idx(self.dataframe.label.iloc[idx])\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYanGf8i4der"
      },
      "outputs": [],
      "source": [
        "train_dataset = Food101(train_imgs, transform=train_transforms)\n",
        "test_dataset = Food101(test_imgs, transform=test_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJARLuu85DE7"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFzKinbY5FsI"
      },
      "outputs": [],
      "source": [
        "for i in range(101):\n",
        "    image = train_dataset.__getitem__(i)\n",
        "    print(encoder.get_label(image[1]), image[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "aSHM2bA8gq_9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02cLQ-GN5KBd"
      },
      "outputs": [],
      "source": [
        "weights = models.DenseNet201_Weights.IMAGENET1K_V1\n",
        "model = models.densenet201(weights = weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ojPr2aL5PI1"
      },
      "outputs": [],
      "source": [
        "# Freeze parameters so we don't backprop through them\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WklmORs5X1x"
      },
      "outputs": [],
      "source": [
        "classifier = nn.Sequential(\n",
        "    nn.Linear(1920,1024),\n",
        "    nn.LeakyReLU(),\n",
        "    nn.Linear(1024,101),\n",
        ")\n",
        "\n",
        "model.classifier = classifier\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVmYN8GR5gUG"
      },
      "outputs": [],
      "source": [
        "num_epochs = 15\n",
        "\n",
        "# loss\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# all parameters are being optimized\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=[0.9, 0.999])\n",
        "\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0MZ6Fu95iKp"
      },
      "outputs": [],
      "source": [
        "def train_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device: torch.device):\n",
        "  # Put model in train mode\n",
        "  model.train()\n",
        "\n",
        "  # Setup train loss and train accuracy values\n",
        "  train_loss, train_acc = 0, 0\n",
        "\n",
        "  print(\"--> Training Progress\")\n",
        "  # Loop through data loader data batches\n",
        "  for batch, (X, y) in enumerate(tqdm(dataloader)):\n",
        "      # Send data to target device\n",
        "      images, labels = X.to(device), y.to(device)\n",
        "\n",
        "      # 1. Forward pass\n",
        "      y_pred = model(images)\n",
        "\n",
        "      # 2. Calculate  and accumulate loss\n",
        "      loss = loss_fn(y_pred, labels)\n",
        "      train_loss += loss.item()\n",
        "\n",
        "      # 3. Optimizer zero grad\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # 4. Loss backward\n",
        "      loss.backward()\n",
        "\n",
        "      # 5. Optimizer step\n",
        "      optimizer.step()\n",
        "\n",
        "      # Calculate and accumulate accuracy metric across all batches\n",
        "      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "      train_acc += (y_pred_class == labels).sum().item()/len(y_pred)\n",
        "\n",
        "  # Adjust metrics to get average loss and accuracy per batch\n",
        "  train_loss = train_loss / len(dataloader)\n",
        "  train_acc = train_acc / len(dataloader)\n",
        "  return train_loss, train_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFOKoY525nJ5"
      },
      "outputs": [],
      "source": [
        "def test_step(model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              device: torch.device):\n",
        "  # Put model in eval mode\n",
        "  model.eval()\n",
        "\n",
        "  # Setup test loss and test accuracy values\n",
        "  test_loss, test_acc = 0, 0\n",
        "\n",
        "  # Turn on inference context manager\n",
        "  with torch.inference_mode():\n",
        "      print(\"--> Testing Progress\")\n",
        "      # Loop through DataLoader batches\n",
        "      for batch, (X, y) in enumerate(tqdm(dataloader)):\n",
        "          # Send data to target device\n",
        "          images, labels = X.to(device), y.to(device)\n",
        "\n",
        "          # 1. Forward pass\n",
        "          test_pred_logits = model(images)\n",
        "\n",
        "          # 2. Calculate and accumulate loss\n",
        "          loss = loss_fn(test_pred_logits, labels)\n",
        "          test_loss += loss.item()\n",
        "\n",
        "          # Calculate and accumulate accuracy\n",
        "          test_pred_labels = torch.argmax(torch.softmax(test_pred_logits, dim=1), dim=1)\n",
        "\n",
        "          test_acc += ((test_pred_labels == labels).sum().item()/len(test_pred_labels))\n",
        "\n",
        "  # Adjust metrics to get average loss and accuracy per batch\n",
        "  test_loss = test_loss / len(dataloader)\n",
        "  test_acc = test_acc / len(dataloader)\n",
        "  return test_loss, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nACsOMec5q2B"
      },
      "outputs": [],
      "source": [
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          epochs: int,\n",
        "          device: torch.device):\n",
        "  # Create empty results dictionary\n",
        "  history = {\n",
        "      \"train_loss\": [],\n",
        "      \"train_acc\": [],\n",
        "      \"test_loss\": [],\n",
        "      \"test_acc\": [],\n",
        "      'best train acc': (0, 0),\n",
        "      \"best_model\": dict()\n",
        "  }\n",
        "\n",
        "  # Loop through training and testing steps for a number of epochs\n",
        "  for epoch in range(epochs):\n",
        "      print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
        "\n",
        "      train_loss, train_acc = train_step(model=model,\n",
        "                                          dataloader=train_dataloader,\n",
        "                                          loss_fn=loss_fn,\n",
        "                                          optimizer=optimizer,\n",
        "                                          device=device)\n",
        "      test_loss, test_acc = test_step(model=model,\n",
        "          dataloader=test_dataloader,\n",
        "          loss_fn=loss_fn,\n",
        "          device=device)\n",
        "\n",
        "      # Print out what's happening\n",
        "      print(\n",
        "          f\"Epoch: {epoch+1} | \"\n",
        "          f\"train_loss: {train_loss:.4f} | \"\n",
        "          f\"train_acc: {train_acc:.4f} | \"\n",
        "          f\"test_loss: {test_loss:.4f} | \"\n",
        "          f\"test_acc: {test_acc:.4f}\"\n",
        "          f\"\\n\\n=============================\\n\"\n",
        "      )\n",
        "\n",
        "      # Update results dictionary\n",
        "      history[\"train_loss\"].append(train_loss)\n",
        "      history[\"train_acc\"].append(train_acc)\n",
        "      history[\"test_loss\"].append(test_loss)\n",
        "      history[\"test_acc\"].append(test_acc)\n",
        "      if test_loss < history[\"test_acc\"][len(history[\"test_acc\"]) - 1]:\n",
        "          history[\"best_model\"] = model.state_dict()\n",
        "\n",
        "      if test_acc > 0.95:\n",
        "         break\n",
        "\n",
        "  # Return the filled results at the end of the epochs\n",
        "  return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "E3KymPRg5_OD"
      },
      "outputs": [],
      "source": [
        "model, history = train(model, train_loader, test_loader, optimizer, loss_fn, num_epochs, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "7bstXAE9gztD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQUlvaid6CvW"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader):\n",
        "\n",
        "  random = np.random.randint(0, len(dataloader))\n",
        "\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "\n",
        "    for images, labels in tqdm(dataloader):\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      outputs = model(images)\n",
        "\n",
        "      preds = torch.argmax(torch.softmax(outputs, 1), 1)\n",
        "\n",
        "      preds = np.array([pred.cpu() if pred < 100 else 100 for pred in preds])\n",
        "      labels = np.array([label.cpu() if label < 100 else 100 for label in labels])\n",
        "\n",
        "      n_samples += labels.shape[0]\n",
        "      n_correct += (preds==labels).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBWKyLwy6GID"
      },
      "outputs": [],
      "source": [
        "evaluate(model,test_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}